# rag.py
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # ç¦ç”¨ Hugging Face çš„å¹¶è¡Œè­¦å‘Š
os.environ["BITSANDBYTES_NOWELCOME"] = "1"      # ç¦æ­¢ bitsandbytes æ¬¢è¿ä¿¡æ¯
os.environ["WANDB_DISABLED"] = "true"           # ç¦ç”¨ wandb æ—¥å¿—ï¼ˆå¦‚æœæœ‰ï¼‰

import faiss
import numpy as np
import pickle
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer
import pandas as pd
from config import config
import logging
from docx import Document  # ä»¥é˜²åé¢éœ€è¦ docx è§£æ
import os
##############################################################################
# 1. åŠ è½½æ¨¡å‹ä¸ç´¢å¼•
##############################################################################

# åŠ è½½æœ¬åœ° Embedding æ¨¡å‹ï¼ˆSentenceTransformerï¼‰ï¼Œç”¨äº FAISS æ£€ç´¢
embedding_model_path = os.path.abspath(config.EMBEDDING_MODEL)
embedding_model = SentenceTransformer(embedding_model_path)

# åŠ è½½ FAISS ç´¢å¼•
index = faiss.read_index(str(config.FAISS_CACHE / "docs.index"))
print(f"âœ… FAISS ç´¢å¼•ç»´åº¦ï¼š{index.d}")

# åŠ è½½æ–‡ä»¶ååˆ—è¡¨
with open(str(config.FAISS_CACHE / "filenames.pkl"), "rb") as f:
    filenames = pickle.load(f)

# åŠ è½½æœ¬åœ° LLMï¼Œå¹¶ä½¿ç”¨ GPU åŠ é€Ÿ
llm_model_path = os.path.abspath(config.LLM_MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(llm_model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    llm_model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

# å¦‚æœæƒ³æ‰‹åŠ¨æŒ‡å®š deviceï¼Œä¹Ÿå¯ç”¨ model.to(config.DEVICE)ï¼Œè¿™é‡Œ device_map="auto" é€šå¸¸å°±èƒ½å·¥ä½œ

print("âœ… æœ¬åœ° LLM æ¨¡å‹åŠ è½½å®Œæˆï¼")

##############################################################################
# 2. æ–‡æ¡£åŠ è½½å‡½æ•°
##############################################################################

def load_document_content(filename):
    """
    æ ¹æ®æ–‡ä»¶ååŠ è½½æ–‡æœ¬å†…å®¹ã€‚identity.md ä½œä¸º AI è§’è‰²è®¾å®šã€‚
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„çŸ¥è¯†åº“ç›®å½•
    file_path = os.path.join(config.REFERENCE_FOLDER, filename)


    # ç‰¹æ®Šå¤„ç† identity.md
    if filename == "identity.md":
        with open(file_path, "r", encoding="utf-8") as file:
            return f"[ç³»ç»Ÿè§’è‰²æç¤º]\n{file.read()}\n\n"

    if filename.endswith(".txt") or filename.endswith(".md"):
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read()
    elif filename.endswith(".pdf"):
        from pypdf import PdfReader
        reader = PdfReader(file_path)
        return "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
    elif filename.endswith(".docx"):
        # åˆ©ç”¨ python-docx è¯»å–
        doc = Document(file_path)
        return "\n".join([para.text for para in doc.paragraphs])
    elif filename.endswith(".xlsx") or filename.endswith(".csv"):
        try:
            df = pd.read_excel(file_path) if filename.endswith(".xlsx") else pd.read_csv(file_path)
            return df.to_string(index=False)
        except Exception as e:
            return f"âŒ è¯»å– {filename} æ—¶å‡ºé”™: {str(e)}"
    elif filename.endswith(".pptx"):
        try:
            from pptx import Presentation
            prs = Presentation(file_path)
            text = []
            for slide in prs.slides:
                for shape in slide.shapes:
                    if shape.has_text_frame:
                        for paragraph in shape.text_frame.paragraphs:
                            text.append(paragraph.text)
            return "\n".join(text)
        except Exception as e:
            return f"âŒ è¯»å– {filename} æ—¶å‡ºé”™: {str(e)}"

    return "âŒ æ— æ³•è¯»å–è¯¥æ–‡æ¡£æ ¼å¼ï¼š" + filename


##############################################################################
# 3. æ£€ç´¢å‡½æ•°
##############################################################################

def retrieve_top_k_documents(query, k=3):
    """
    æ ¹æ®æŸ¥è¯¢è¯­å¥åœ¨ç´¢å¼•ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„ k ä¸ªæ–‡æ¡£ï¼Œå¹¶è¿”å›å…¶å†…å®¹ï¼ˆæˆªå–ï¼‰ã€‚
    """
    query_embedding = np.array(embedding_model.encode([query]))

    print(f"âœ… æŸ¥è¯¢å‘é‡ç»´åº¦ï¼š{query_embedding.shape[1]}")

    _, idxs = index.search(query_embedding, k)

    retrieved_docs = []
    for i in idxs[0]:
        filename = filenames[i]
        content = load_document_content(filename)
        # è¿™é‡Œå¯ä»¥åªæˆªå–å‰1000å­—ç¬¦ï¼Œé¿å… Prompt è¿‡é•¿
        retrieved_docs.append(f"ğŸ“„ã€{filename}ã€‘\n{content[:100]}...")
    return retrieved_docs


##############################################################################
# 4. åˆ†å— (Chunk) + æ‘˜è¦ / æ”¹å†™
##############################################################################

def chunk_text(text, chunk_size=1000, overlap=200):
    """
    å°†é•¿æ–‡æœ¬åˆ†å—ï¼Œæ¯å— chunk_size ä¸ªå­—ç¬¦ï¼Œå¹¶åœ¨å—ä¹‹é—´ä¿ç•™ overlap ä¸ªå­—ç¬¦é‡å ï¼Œé¿å…å…³é”®ä¿¡æ¯è¢«åˆ‡æ–­ã€‚
    """
    chunks = []
    start = 0
    text_len = len(text)
    while start < text_len:
        end = min(start + chunk_size, text_len)  # é¿å…è¶Šç•Œ
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap

        if start < 0:
            start = 0
        if start >= text_len:
            break
    return chunks

def summarize_long_text(text):
    """
    å¯¹é•¿æ–‡æœ¬è¿›è¡Œå¤šæ®µå¼æ‘˜è¦ï¼Œç„¶ååˆå¹¶ã€‚
    """
    # åˆ†å—
    text_chunks = chunk_text(text, chunk_size=1500, overlap=200)
    chunk_summaries = []

    # é€å—æ‘˜è¦
    for idx, chunk in enumerate(text_chunks):
        prompt = f"è¯·é˜…è¯»ä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼Œå¹¶è¿›è¡Œç®€è¦æ€»ç»“ï¼š\n{chunk}\n\næ€»ç»“ï¼š"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        output = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
        summary_chunk = tokenizer.decode(output[0], skip_special_tokens=True)
        chunk_summaries.append(summary_chunk.strip())

    # åˆå¹¶æ‰€æœ‰å—çš„æ‘˜è¦ï¼Œå†è®©æ¨¡å‹åšä¸€æ¬¡â€œæ€»æ€»ç»“â€
    combined_summary = "\n".join(chunk_summaries)
    final_prompt = f"ä»¥ä¸‹æ˜¯å¤šä¸ªåˆ†å—çš„æ€»ç»“ï¼Œè¯·å°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªç®€æ´çš„æ•´ä½“æ€»ç»“ï¼š\n{combined_summary}\n\næ•´ä½“æ€»ç»“ï¼š"
    final_inputs = tokenizer(final_prompt, return_tensors="pt").to(model.device)
    final_output = model.generate(
        **final_inputs,
        max_new_tokens=500,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    final_summary = tokenizer.decode(final_output[0], skip_special_tokens=True)
    return final_summary.strip()

def rewrite_long_text(text):
    """
    å¯¹é•¿æ–‡æœ¬è¿›è¡Œâ€œæ”¹å†™ / æ¶¦è‰²â€ï¼Œä¸æ‘˜è¦ç±»ä¼¼çš„æ€è·¯ï¼Œå…ˆåˆ†å—å†åˆå¹¶ã€‚
    """
    # å…ˆç²—æš´ç¤ºä¾‹ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦æ‹†åˆ†åšå¤šæ¬¡æ”¹å†™
    text_chunks = chunk_text(text, chunk_size=1500, overlap=200)
    rewrite_results = []

    for chunk in text_chunks:
        prompt = f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œè¯­è¨€æ¶¦è‰²æˆ–æ”¹å†™ï¼Œä½¿å…¶æ›´é€šé¡ºã€ç®€æ´ï¼š\n{chunk}\n\næ”¹å†™åï¼š"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        output = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
        rewrite_chunk = tokenizer.decode(output[0], skip_special_tokens=True)
        rewrite_results.append(rewrite_chunk.strip())

    # ç®€å•æ‹¼æ¥ï¼Œå¦‚æœæƒ³å†åšæœ€ç»ˆåˆå¹¶ï¼Œå¯ä»¥å†æ¥ä¸€æ¬¡ç”Ÿæˆ
    return "\n".join(rewrite_results).strip()


##############################################################################
# 5. æœ€ç»ˆå›ç­”ï¼šgenerate_answer
#    - å¦‚æœç”¨æˆ·é—® "æ€»ç»“xxæ–‡ä»¶" æˆ– "æ”¹å†™xxæ–‡ä»¶"ï¼š
#         -> ç›´æ¥æ‰¾åˆ°æ–‡ä»¶å†…å®¹åš summarize/rewrite
#    - å¦åˆ™åšæ™®é€šRAGé—®ç­”
##############################################################################

def generate_answer(query):
    # 1) æ£€ç´¢æ–‡æ¡£
    related_docs = retrieve_top_k_documents(query, k=3)

    # 2) åŠ è½½ identity.mdï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    identity_content = ""
    if "identity.md" in filenames:
        identity_content = load_document_content("identity.md")

    # 3) æ‹¼æ¥ä¸Šä¸‹æ–‡
    context = f"AI Identity/Persona\n{identity_content}\n\nã€çŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘\n" + "\n\n".join(related_docs)

    # æ„é€  Prompt
    prompt = f"""
ä½ æ˜¯ä¸€åæ™ºèƒ½é—®ç­”åŠ©æ‰‹(æ‰®æ¼”DeepSeekçŸ¥è¯†ç®¡å®¶ äººå·¥æ™ºèƒ½åŸç†ç¬¬å››ç»„)ï¼Œä»¥ä¸‹æ˜¯ä½ çš„èº«ä»½æè¿°å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:
{context}

è¯·éµå®ˆidentity.mdä¸­çš„æ‰€æœ‰â€œæ²Ÿé€šé£æ ¼å‡†åˆ™â€å’Œâ€œç‰¹æ®Šè¡Œä¸ºå‡†åˆ™â€ï¼Œå¹¶æ ¹æ®æ–‡æ¡£åšå‡ºå›ç­”ã€‚
å¦‚æœåœ¨çŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·å›ç­”â€œå¯¹ä¸èµ·ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚
è¯·ä½¿ç”¨ç®€æ´ä¸”ä¸“ä¸šçš„å£å»ã€‚
ç”¨æˆ·çš„é—®é¢˜ï¼š{query}

è¯·ç›´æ¥ç»™å‡ºç®€æ´ä¸”ä¸“ä¸šçš„å›ç­”ï¼š
""".strip()

    # æµå¼ç”Ÿæˆå›ç­”
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # ä½¿ç”¨æ¨¡å‹çš„æµå¼ç”Ÿæˆå‚æ•°ï¼ˆéœ€transformers >=4.21.0ï¼‰
    output_stream = model.generate(
        **inputs,
        max_new_tokens=300,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True
    )

    # åˆå§‹åŒ–è¾“å‡º
    generated_text = ""

    # æµå¼é€tokenè¾“å‡º
    for token_id in output_stream[0]:
        token_text = tokenizer.decode(token_id, skip_special_tokens=True)
        generated_text += token_text
        yield generated_text  # å®æ—¶è¿”å›å½“å‰ç”Ÿæˆå†…å®¹

    # å®Œæ•´çš„è°ƒè¯•ä¿¡æ¯ï¼ˆç”Ÿæˆå®Œæˆåï¼‰
    debug_info = (
        "\n\n### æ£€ç´¢ä¸æ¨ç†è¿‡ç¨‹\n\n"
        f"**ç”¨æˆ·é—®é¢˜**: {query}\n\n"
        f"**Prompt å†…å®¹**: \n{prompt}\n\n"
        "â€”â€”ä»¥ä¸Šä¿¡æ¯ä»…ä¾›è°ƒè¯•æˆ–è¿›é˜¶æŸ¥çœ‹â€”â€”\n"
    )

    # æœ€ç»ˆè¾“å‡ºå¸¦æœ‰è°ƒè¯•ä¿¡æ¯ï¼ˆåŠ ä¸Š</think>æ ‡è®°ï¼‰
    #print(f"{generated_text}</think>{debug_info}")
    yield f"{generated_text}</think>{debug_info}"



##############################################################################
# 6. ç®€å•å‡½æ•°ï¼š_simple_summarize / _simple_rewrite (å¯¹çŸ­æ–‡æœ¬)
##############################################################################

def _simple_summarize(text):
    """
    å¯¹çŸ­æ–‡æœ¬åšä¸€æ¬¡æ€§æ‘˜è¦ã€‚å¦‚æœæ–‡æœ¬ä¸å¤§ï¼Œå¯ç›´æ¥ç”¨ã€‚
    """
    prompt = f"è¯·é˜…è¯»ä»¥ä¸‹æ–‡æœ¬å¹¶è¿›è¡Œç®€è¦æ€»ç»“ï¼š\n{text}\n\næ€»ç»“ï¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(
        **inputs,
        max_new_tokens=300,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output[0], skip_special_tokens=True).strip()

def _simple_rewrite(text):
    """
    å¯¹çŸ­æ–‡æœ¬åšä¸€æ¬¡æ€§æ”¹å†™ã€‚
    """
    prompt = f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œè¯­è¨€æ¶¦è‰²æˆ–æ”¹å†™ï¼Œä½¿å…¶æ›´é€šé¡ºã€ç®€æ´ï¼š\n{text}\n\næ”¹å†™åï¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(
        **inputs,
        max_new_tokens=300,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output[0], skip_special_tokens=True).strip()


##############################################################################
# 7. å‘½ä»¤è¡Œäº¤äº’å…¥å£
##############################################################################


import re

# if __name__ == "__main__":
#     print("ğŸ“š RAG + Summarize/Rewrite ç¤ºä¾‹ç¨‹åºå¯åŠ¨...")
#     try:
#         while True:
#             query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š ")
#             if query.lower().strip() == "exit":
#                 print("\nğŸ‘‹ é€€å‡º RAG ç³»ç»Ÿï¼Œå†è§ï¼")
#                 break
#
#             # åˆå§‹åŒ–å˜é‡
#             bot_response = ""
#             debug_info = "æš‚æ— æ¨ç†è¿‡ç¨‹"
#             current_output_buffer = ""
#
#             # æµå¼å¤„ç†ç”Ÿæˆå™¨è¾“å‡º
#             stream = generate_answer(query)
#             for current_output in stream:
#                 current_output_buffer += current_output
#                 if "</think>" in current_output_buffer:
#                     parts = current_output_buffer.split("</think>")
#                     if len(parts) >= 2:
#                         bot_response = parts[1].strip()  # æ¨¡å‹çš„å›ç­”
#                         if len(parts) > 2:
#                             debug_info = parts[2].strip()  # è°ƒè¯•ä¿¡æ¯
#                         else:
#                             debug_info = "æš‚æ— æ¨ç†è¿‡ç¨‹"
#                         # æ¸…ç©ºç¼“å†²åŒº
#                         current_output_buffer = ""
#                         # æ‰“å°æµå¼è¾“å‡ºçš„å›ç­”
#                         print("\nAI å›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š", bot_response, "\n")
#                     else:
#                         bot_response = parts[0].strip()
#                         current_output_buffer = ""
#                 else:
#                     # å¦‚æœæ²¡æœ‰æ‰¾åˆ° </think>ï¼Œç»§ç»­ç§¯ç´¯è¾“å‡º
#                     continue
#
#             # å¦‚æœç¼“å†²åŒºä¸­è¿˜æœ‰å†…å®¹ï¼Œå¤„ç†å‰©ä½™éƒ¨åˆ†
#             if current_output_buffer:
#                 bot_response = current_output_buffer.strip()
#                 print("\nAI å›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š", bot_response, "\n")
#
#     except KeyboardInterrupt:
#         print("\nğŸ‘‹ æ£€æµ‹åˆ° Ctrl + Cï¼Œé€€å‡º RAG ç³»ç»Ÿï¼")
if __name__ == "__main__":
    print("ğŸ“š RAG + Summarize/Rewrite ç¤ºä¾‹ç¨‹åºå¯åŠ¨...")
    try:
        while True:
            query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š ")
            if query.lower().strip() == "exit":
                print("\nğŸ‘‹ é€€å‡º RAG ç³»ç»Ÿï¼Œå†è§ï¼")
                break

            # åˆå§‹åŒ–å˜é‡
            bot_response = ""
            debug_info = "æš‚æ— æ¨ç†è¿‡ç¨‹"

            # æµå¼å¤„ç†ç”Ÿæˆå™¨è¾“å‡º
            stream = generate_answer(query)
            for current_output in stream:
                if "</think>" in current_output:
                    parts = current_output.split("</think>")
                    if len(parts) >= 2:
                        bot_response = parts[1].strip()  # æ¨¡å‹çš„å›ç­”
                        if len(parts) > 2:
                            debug_info = parts[2].strip()  # è°ƒè¯•ä¿¡æ¯
                    else:
                        bot_response = parts[0].strip()
                else:
                    bot_response = current_output.strip()

            # è¾“å‡ºæœ€ç»ˆå›ç­”
            print("\nAI å›ç­”ï¼š", bot_response, "\n")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ£€æµ‹åˆ° Ctrl + Cï¼Œé€€å‡º RAG ç³»ç»Ÿï¼")

